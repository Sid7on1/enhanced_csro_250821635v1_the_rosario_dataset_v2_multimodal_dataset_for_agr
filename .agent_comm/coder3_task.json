{
  "agent_id": "coder3",
  "task_id": "task_1",
  "files": [
    {
      "name": "data_loader.py",
      "purpose": "Image data loading and batching",
      "priority": "high"
    },
    {
      "name": "config.py",
      "purpose": "Model configuration",
      "priority": "high"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.RO_2508.21635v1_The_Rosario_Dataset_v2_Multimodal_Dataset_for_Agr",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.RO_2508.21635v1_The-Rosario-Dataset-v2-Multimodal-Dataset-for-Agr with content analysis. Detected project type: computer vision (confidence score: 7 matches).",
    "key_algorithms": [
      "Fusion",
      "Obstacle",
      "Each",
      "Ackermann",
      "Variance",
      "Problem",
      "Calibration",
      "Mobile",
      "Mapping",
      "Point"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.RO_2508.21635v1_The-Rosario-Dataset-v2-Multimodal-Dataset-for-Agr.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nThe Rosario Dataset v2: Multimodal\nDataset for Agricultural RoboticsSoncini N, Cremona J, Vidal E, Garc \u00b4\u0131a\nM, Castro G, Pire T. The Rosario\ndataset v2: Multi-modal dataset for\nagricultural robotics. The International\nJournal of Robotics Research. 2025;0(0).\ndoi:10.1177/02783649251368909\nSAGE\nNicol \u00b4as Soncini1,*, Javier Cremona1,*, Erica Vidal1, Maximiliano Garc\u00b4 \u0131a1, Gast \u00b4on Castro2,\nTaih \u00b4 u Pire1\nAbstract\nWe present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from\nsensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point\nPositioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. This dataset captures key\nchallenges inherent to robotics in agricultural environments, including variations in natural lighting, motion blur, rough\nterrain, and long, perceptually aliased sequences. By addressing these complexities, the dataset aims to support\nthe development and benchmarking of advanced algorithms for localization, mapping, perception, and navigation in\nagricultural robotics. The platform and data collection system is designed to meet the key requirements for evaluating\nmulti-modal SLAM systems, including hardware synchronization of sensors, 6-DOF ground truth and loops on long\ntrajectories.\nWe run multimodal state-of-the art SLAM methods on the dataset, showcasing the existing limitations in their application\non agricultural settings. The dataset and utilities to work with it are released on https://cifasis.github.io/\nrosariov2/ .\nKeywords\ndataset, multi-modal, agriculture, crop field, ground-truth, soybean field, unstructured\n1 Introduction\nWorld population will grow by a third by 2050, directly\nimpacting global food demand (Fukase and Martin (2020)).\nIn this context, the agricultural industry should increase its\nproduction to satisfy such demand. The use of autonomous\nrobots to carry out agricultural tasks such as seeding,\nharvesting, weed remotion, pest control among others is an\nattractive solution since it can improve the production time\nin a sustainable manner reducing the environmental impact\nand pollution.\nHowever, the implementation of autonomous robots in\nthe agricultural field is a challenging work due the rough\nterrain, natural light variations, perceptual aliasing, areas\nwith GNSS-denied signal, and the long-term robot operation\nrequired to carry out the desired applications. All these\nchallenges cause robot localization methods to fail or\nperform poorly, making them impractical for real agricultural\ntasks, as evidenced in Cremona et al. (2022, 2023); Soncini\net al. (2024); Cox et al. (2023); Bai et al. (2023); Ait et al.\n(2023).In the last decade, there has been a growing trend towards\nthe creation and public availability of agricultural datasets,\nenabling researchers to test new techniques and develop\nmore sophisticated algorithms to address these challenges,\nsuch as Pire et al. (2019); Kragh et al. (2017); Tanco et al.\n(2024). However, none of them are properly curated for\nevaluating multi-modal SLAM algorithms. Effective multi-\nmodal SLAM evaluation imposes specific requirements such\nas hardware-synchronized sensors, 6-DOF ground-truth,\nand trajectories with loops to effectively test loop closure\nalgorithms.\nIn this work, we present a multi-modal dataset recorded by\nthe weed removing robot developed at CIFASIS (CONICET-\nUNR) in a soybean agricultural field. The recorded data\n1CIFASIS (CONICET -UNR), Rosario, Santa Fe, Argentina\n2Universidad de San Andr \u00b4es (UDESA-CONICET), Buenos Aires,\nArgentina\n*Equally contributing authors\nCorresponding author:\nNicol \u00b4as Soncini, CIFASIS (CONICET -UNR), Bv. 27 de Febrero 210 bis,\nS2000EZP , Rosario, Santa Fe, Argentina\nEmail: soncini@cifasis-conicet.gov.ar\nPrepared using sagej.cls [Version: 2017/01/17 v1.20]arXiv:2508.21635v1  [cs.RO]  29 Aug 2025\n\n--- Page 2 ---\n2 Journal Title XX(X)\nFigure 1. Photos of the weed removing robot on the field and a\nclose-up of the sensor suite it carries on its front.\nis captured by a RGB camera, stereo IR camera, a 6-Axis\nIMU, three 9-Axis IMU, three GNSS receivers and wheel\nencoders, and we provide a synchronization system for the\ndata produced. The dataset consists of six sequences where\nthe robots covers a total of 7.3 km in2.2 h. The recorded\ndata captured natural light changes, image blurred, long\ntrajectories, loop closures, rough terrain navigation. In order\nto illustrate the dataset challenges, we evaluate and analyze\nmulti-modal state of the art SLAM systems on it. The dataset\nwas conceived to facilitate the development of new multi-\nmodal SLAM algorithms for agricultural environments. We\nalso provide a 6-DoF ground-truth obtained by two GNSS\nand inertial measurements.\nThe main contributions of this works can be summarized\nin the following:\n\u2022 Multi-sensor data captured by RGB camera, stereo IR\ncamera, a 6-Axis IMU, three 9-Axis IMU, three GNSS\nreceivers and wheel encoders.\n\u2022 6-DoF pose ground-truth generated by two GNSS and\ninertial measurements.\n\u2022 Trajectories with loops, which are trajectories where\nthe robot revisits the same location after traveling\nalong a certain path.\n\u2022 A set of tools used to handle the data, provided as\nopen-source.\n\u2022 Sensor calibration sequences.\n\u2022 Data is provided in both ROS rosbags and plain files.\nThe dataset and the development tools are release as open-\nsource to facilitate the data usage and the development of\nnew SLAM multi-modal systems.\nWe organize the rest of the article as follows: in\nSection 2 we present the most recent and relevant work\nrelated to datasets for agricultural robotics. In Section 3 wedescribe the physical setup of the robot used to record the\npresent dataset as well as the software and the calibration\nmethodology applied. In Section 4 we showcase the recorded\ndata released in the dataset, along with information about\nthe recording methodology and pre and post-processing\nof the data. In Section 5 we analyze results of running\nstate-of-the-art algorithms for localization, mapping and\n3D reconstruction on the data in an attempt to prove its\nworthiness and show what still remains to be addressed\nfor enhancing these methods on agricultural environments.\nFinally, Section 6 summarizes the presented article and\ndiscusses future work.\n2 Related Work\nIn the field of agriculture there are many types of crops,\nterrains and farming practices, each creating different visual\nimprints and navigation patterns. Our work continues what\nstarted in Pire et al. (2019) as it uses the same robot platform\nand navigates similar conditions (a soybean plantation in\nthe form of rows), but we improve the quality and diversity\nof sensors, and address long-term trajectories with loops.\nParticularly, we have replaced the Stereolabs ZED camera,\nconsisting of a stereo pair of rolling-shutter color cameras,\nfor an Intel D435i RGB-D, comprising a stereo pair of\ninfrared (IR) global-shutter cameras, a rolling-shutter color\ncamera and an inertial measurement (IMU) unit. A set of\ndouble-band (L1/L2) GNSS modules was added as well,\nto enhance the positioning for ground-truth calculation and\nprovide an estimate of platform orientation, which was\nmissing in the previous dataset. Improvements were made\nto the sensor synchronization as well, which we elaborate on\nin coming sections.\nIn Table 1, we survey the most recent and relevant\npublicly available datasets focused on robot navigation and\nlocalization in agricultural environments. In order to put\nthe proposed dataset in context of the state of the art, we\nanalyzed each dataset according the type of environment,\nsensors captured, software/hardware synchronization and\nground-truth type. Datasets like Sugar Beets (Chebrolu\net al. (2017)), Ladybird Cobbity Brassica (Bender et al.\n(2020)) and RumexWeeds (G \u00a8uldenring et al. (2023)) provide\nonly downwards facing cameras, which makes them very\nconvenient for weed detection and classification tasks, but\nvery poor in terms of localization and mapping. Others\nlike FieldSAFE (Kragh et al. (2017)) and The GREENBOT\nDataset (Ca \u02dcnadas-Ar \u00b4anega et al. (2024)) contain very good\nmultisensor data on the field, but provide no odometry\ninformation, which can be useful to compensate for IMU\nPrepared using sagej.cls\n\n--- Page 3 ---\nThe Rosario Dataset v2 3\ndrift or continue the position tracking when other sensors\nfail to do so. Some notable datasets like Bacchus Long-\nTerm (Polvara et al. (2023)), MAgro (Polvara et al. (2023)),\nBotanicGarden (Liu et al. (2024)), Terrasenta Under-Canopy\n(Cuaran et al. (2024)) and ARD-VO (Crocetti et al. (2023))\nalso contain high quality multimodal data in agricultural\nsettings, but work in settings where the crops or plants\nraise higher than their sensors, which present a different\nset of problems than working over the crops as they form\ncorridor-like structures around the robot. Another dataset to\nmention is WeedMap (Sa et al. (2018)), which captures all\ndata from a multispectral camera mounted on a drone to\ngenerate composite images or orthomosaics and allows for\ncrop analysis from this data. In our dataset we attempt to\ncover most of the weak points we mention, to provide a rich\nand heterogeneous set of data to further the research on these\nenvironments.\n3 Platform\nThis section describes the physical setup of sensors and\nactuators in the robotic platform as well as details of\nthe software used to capture the data and the calibration\nmethodology applied.\n3.1 The Robot\nThe robot consists of a mobile platform with four\nindependently driven wheels as depicted in Figure 1. It\nhas been designed with autonomy in agricultural settings\nin mind, and as such it\u2019s powered by batteries charged by\na series of solar panels in the top of the vehicle, and the\ndistance and height of the wheels is consistent with the row\ndistance and height of certain crops such as soy.\nThe aim of the robot as a whole is to automate the weeding\nof large crop fields. Particularly we aim to automate the\nrobot\u2019s traversal through a cultivated field whilst classifying\nthe images and applying removal techniques for weeds\nor enriching the crops with fertilizer (things currently in\ntesting). The robot motion is controlled by four independent\nbrushless motors (one per wheel). For the front wheels\ndirection, a stepper motor has been built with the appropriate\nreduction and encoder. For this dataset the motion of the\nplatform was performed by a human operator on-site with\na radio-frequency controller.\n3.2 Data Collection System Overview\nFigure 2 shows the sensor\u2019s placement in the box mounted at\nthe front of the robot. The exact positioning, orientation and\nframes of reference are available in the files of the datasetor the repository. The setup consists of the sensors listed in\nTable 2, which provide both exteroceptive and proprioceptive\ndata on the platform\u2019s environment and state.\nE m l i d  R e a c h  M 1G N S S  A n t e n n a\nE m l i d  R e a c h  M 2G N S S  A n t e n n a\nE m l i d  R e a c h  M 2G N S S  A n t e n n a\nE m l i d  R e a c h M 1  I M U\nE m l i d  R e a c h M 2  I M U\nE m l i d  R e a c h M 2  I M U\nR e a l S e n s e d 4 3 5 i  I M U  a n dC a m e r a s\nFigure 2. Placement of the different sensors in the sensor box.\nTo capture the data, the sensors are connected to a\nNVIDIA Jetson Orin Nano 8 GB Development Kit computer\nwith the following connections:\n\u2022 the Intel Realsense D435i is connected to a USB-A 3.0\nport,\n\u2022 the Reach GNSS modules (all three) are connected to\na powered USB 3.0 hub, that in turn is connected to a\nUSB-A 3.0 port,\n\u2022 a single Emlid Reach GNSS module from the above\n(an M2 module) is connected also to a set of GPIO pins\nto provide a PPS ( Pulse-per-second ) timing capability.\nA diagram of the connections can be seen in Figure 3.\nWe also make use of a Emlid Reach M1 GNSS module\nas a static base for our RTK solution, placing it close to the\nfield we were working on. The base sends RTK corrections,\ntransmitted to the robot\u2019s GNSS modules through a long\nrange Wi-Fi setup, to improve the accuracy of our GNSS\npositioning.\n3.3 Software Environment\nThe data capture system is powered by ROS, where a set\nof ROS Nodes transform the raw sensor data into ROS\nMessages and then collects it into a ROS Bag.\nWe used the official package *provided by the manu-\nfacturer to convert the Intel Realsense D435i images and\nIMU measurements to ROS. For the Reach GNSS sensors, a\nslightly modified version of the Reach RS Driver node\u2020does\n\u2217https://github.com/IntelRealSense/realsense-ros\n\u2020https://github.com/enwaytech/reach_rs_ros_driver\nPrepared using sagej.cls\n\n--- Page 4 ---\n4 Journal Title XX(X)\nDataset Purpose Environment Crop Type Cameras IMU GNSS OdometryGround\n-TruthSynchronization\nFieldSAFE\nKragh et al. (2017)Obstacle\ndetectionOutdoors;\nOver crop;\nGrassGrassStereo RGB;\n360 RGB;\nThermalYesIMU-Fused\nRTKNoPosition;\nOrientationHardware +\nSoftwre\nSugar Beets\nChebrolu et al. (2017)Weed Detection;\nLocalization;\nMappingOutdoors;\nOver crop;\nCrop rowsSugar BeetRGB;\nIR Depth;\nMultispectralNo PPP; RTK Yes Position Software\nWeedMap\nSa et al. (2018)Weed Detection;\nMappingOutdoors;\nOver crop;\nCrop rowsSugar Beet Multispectral Yes Single-Point No Position -\nThe Rosario Dataset\nPire et al. (2019)Localization;\nMappingOutdoors;\nOver crop;\nCrop rowsSoybean Stereo RGB Yes RTK Yes Position Software\nLadybird Cobbity Brassica\nBender et al. (2020)Localization;\nMapping;\nClassificationOutdoors;\nOver crop;\nCrop rowsCauliflower;\nBroccoliStereo RGB;\nThermal;\nMultispectralYesIMU-Fused\nRTKNoPosition;\nOrientation-\nRumexWeeds\nG\u00a8uldenring et al. (2023)Weed DetectionOutdoors;\nOver crop;\nGrassGrass RGB Yes Yes Yes Position Software\nARD-VO\nCrocetti et al. (2023)Localization;\nMappingOutdoors;\nUnder crop;\nTreesLocalization;\nMappingStereo RGB;\nMultispectralYes RTK YesPosition;\nOrientationSoftware\nBacchus Long-Term\nPolvara et al. (2023)Localization;\nMappingOutdoors;\nUnder crop;\nTreesGrapevineRGB;\nDepthYes RTK Yes Position Software\nTerrasentia Under-Canopy\nCuaran et al. (2024)Localization;\nMappingOutdoors;\nUnder crop;\nCrop rowsCorn;\nSoybeanStereo RGB Yes Single-Point Yes Position Software\nMAgro\nTanco et al. (2024)Localization;\nMappingOutdoors;\nUnder crop;\nTreesApple;\nPearStereo RGB Yes RTK Yes Position Software\nBotanicGarden\nLiu et al. (2024)Localization;\nMapping;\nSegmentationOutdoors;\nUnder crop;\nTreesUnspecified Stereo RGB Yes No YesPosition;\nOrientationHardware\nThe GREENBOT Dataset\nCa\u02dcnadas-Ar \u00b4anega et al. (2024)Localization;\nMappingIndoors;\nUnder crop;\nCrop rowsTomato Stereo RGB Yes No No - -\nOursLocalization;\nMappingOutdoors;\nOver crop;\nCrop rowsSoybeanStereo IR;\nRGB; DepthYesSingle-Point;\nRTK; PPKYesPosition;\nOrientationHardware +\nSoftware\nTable 1. Publicly available datasets that target robot navigation and localization in agricultural environments (non-synthetic). A\ndash (-) means that information could not be obtained about that particular characteristic.\nName SensorResolution /\nRangeAcquisition Rate\nIntel Realsense D435iStereo IR Camera1280px \u00d7 720px\n87\u00b0 \u00d7 58\u00b015 Hz\nColor Camera1280px x 720px\n69\u00b0 \u00d7 42\u00b015 Hz\n6-DoF IMU\u00b14 g\n\u00b11000 deg /s200 Hz\nEmlid Reach M1GNSS \u00b1 2.5 m 5 Hz\nRTK-GNSS \u00b1 0.04 m 5 Hz\n9-DoF IMU\u00b18 g\n\u00b11000 deg /s200 Hz\nEmlid Reach M2 (1)GNSS \u00b1 2.5 m 5 Hz\nRTK-GNSS \u00b1 0.04 m 5 Hz\n9-DoF IMU\u00b18 g\n\u00b11000 deg /s200 Hz\nEmlid Reach M2 (2)GNSS \u00b1 2.5 m 5 Hz\nRTK-GNSS \u00b1 0.04 m 5 Hz\n9-DoF IMU\u00b18 g\n\u00b11000 deg /s200 Hz\nE-Bike Wheel\nOdometerHall-Effect Odometry \u00b1 7.5\u25e610 Hz\nOMRON\nE6CP-A8-bit Absolute Encoder\u00b11.4\u25e6\n92\u25e6 10 Hz\nTable 2. List of sensors mounted on the platform and their characteristics.\nPrepared using sagej.cls\n\n--- Page 5 ---\nThe Rosario Dataset v2 5\nthe transformation from NMEA sentences into the standard\nROS NavSatFix Message. There is no official software to\nobtain IMU data from the Reach modules, so we run RTIM-\nULIB2\u2021on the modules to retrieve raw data (accelerometer,\ngyroscope and magnetometer) and transmit it to the host,\nwhich transforms that information into the corresponding\nROS messages.\nAs for the odometry we record Hall effect sensor pulses for\neach of the four wheels and the steering system. The motors\u2019\nmicrocontroller transmits to the host the instant velocity of\nthe wheels in rpm, the angle sensed by the encoder and the\nmovement\u2019s direction (forward/backward). To measure the\nangle of rotation of the wheels, a variable of interest for\nvehicle control, an 8-bit absolute encoder (OMRON E6CP-\nA) was used, mounted on the same stepper motor. With\nthat information the wheel odometry can be obtained by\nthe Ackermann model Mueller (2019). The wheelbase of\nour robot is 1.65 m , the steering angle \u2208[\u22120.72,0.58] rad\nand the wheel diameter 0.57 m . The dataset includes the\npost-processed odometry, robot linear and angular velocities,\nand the raw data. The embedded system controls the wheel\nmotors and their steering.\n3.4 Synchronization\nJetson\nGPSD\nEmlid \nReach M2\nRealSense d435i\nGPIO\nUSB\nUSB\nWheel Encoders\nUSB\nSystem\nClock\nChrony\nEmlid \nReach M2\nEmlid \nReach M1\nUSB\nUSB\nNMEA\n+ IMU\nPPS\nNMEA + IMU\nNMEA + IMU\nFigure 3. Synchronization scheme, where a single Emlid\nReach M2 sensor is used with its PPS signal to synchronize the\nJetson\u2019s OS clock with the GNSS UTC time. This time reference\nis then propagated to the RealSense D435i allowing referencing\nacquired sensor data with respect to a unique time source.\nIn isolated environments, such as agricultural fields,\nmobile network coverage is often unavailable. In such\nscenarios, it is required to time synchronize the integrated\ncomputing system with the GNSS modules in order to\nreference acquired sensor data with respect to a unique time\nsource, as GNSS modules are able to converge with the\nsatellite\u2019s Coordinated Universal Time (UTC). The GNSS\nUTC time information (NMEA message) provided by one\nof the Emlid Reach GNSS M2 module is combined with\nits PPS (Pulse-per-Second) signal to set the clock of the\nLinux system\u00a7running in the NVIDIA Jetson module.\nThe Linux OS uses the \u201cChrony\u201d\u00b6Network Time Protocol(NTP) implementation for this purpose, which makes the\nsystem time to gradually converge with the GNSS time\nby slowing down or speeding up the clock as required\ntaking into account the GNSS time, the PPS signal and\ntransmission latency that might exist. Figure 3 depicts the\ndescribed data synchronization scheme. This configuration\nallows precise clock synchronization that is forwarded to\nthe entire software environment aforementioned. In this way,\nROS nodes directly timestamp and record messages in UTC\nreference. It is worth mentioning that inertial information\nprovided by IMUs inside the Emlid Reach modules are\ndirectly generated in UTC time reference. Furthermore, the\nIntel RealSense D435i sensor unit is time synchronized with\nthe Jetson system clock making use of its \u201cGlobal Time\nDomain\u201d driver feature which adjusts the D435i sensor clock\nconsidering the USB transmission latency. This allows the\nD435i sensor to directly generate data frames timestamped in\nthe UTC time reference. The wheel odometry sensors are the\nonly ones whose data is only synchronized by software, this\nis, we timestamp the hall-effect readings when they arrive to\nthe Jetson module and are subsequently converted into ROS\nmessages.\nThis schema improves over the purely software-based\nsynchronization present in the previous Rosario Dataset,\ngiven that it allows all sensor readings to easily be\nsynchronized with one another and with the readings of\nthe GNSS measurements. This also makes it straightforward\nto compare results obtained with a SLAM system against\nthe ground-truth data provided by the GNSS measurements,\nwithout the need to manually adjust the timing by matching\nmeasurements of different time sources.\n3.5 Calibration\n3.5.1 IMU Intrinsic Calibration: IMUs suffer from various\nstochastic errors, such as bias instability and random walk,\nwhich can degrade the accuracy of navigation and mapping\napplications. Intrinsic calibration of IMUs is required to\ndetermine internal sensor parameters like scale factors and\naxis misalignments, crucial for accurate motion tracking,\nespecially in applications requiring high precision sensor\nperformance. Allan variance is a statistical tool widely\nused to assess the stability of sensor measurements over\ntime, helping to identify noise characteristics and systematic\nerrors. The Allan variance is computed by segmenting the\n\u2021https://github.com/HongshiTan/RTIMULib2\n\u00a7Ubuntu 18.04.6 LTS (GNU/Linux 4.9.140-tegra aarch64)\n\u00b6https://chrony-project.org/\nPrepared using sagej.cls\n\n--- Page 6 ---\n6 Journal Title XX(X)\nIMU data into Nclusters of a given averaging time \u03c4.\nThen, the average values of the signal over each segment is\ncomputed. The Allan Variance is then given by:\n\u03c32(\u03c4) =1\n2 (n\u22121)N\u22121X\ni=1(\u00afxi+1\u2212\u00afxi)2, (1)\nwhere \u00afxiare the averaged values over segments Woodman\n(2007). We focus on its square root, the Allan deviation \u03c3.\nTo identify different noise types, we plot \u03c3as a function of \u03c4\non a log-log scale, where distinct noise characteristics appear\nas specific slope patterns IEEE-Standard (1998). Figure 4\npresents an example of this type of plot. We aim to extract the\nfollowing parameters: gyroscope white noise (also known as\ngyroscope noise density or angular random walk), gyroscope\nrandom walk, accelerometer white noise (also known as\naccelerometer noise density or velocity random walk), and\naccelerometer random walk. White noise parameters are\nobtained by fitting a line to the region of the curve where\nthe slope is \u22121\n2and reading its value at \u03c4= 1. Conversely,\ngyroscope and accelerometer random walk parameters are\ndetermined by fitting a line in the region where the slope is\n1\n2and reading its value at \u03c4= 3.\nBias \nInstabilityAngle/Velocity\nRandom\nWalkRate RandomWalk-1/2\n1/2\n0\nFigure 4. Allan deviation \u03c3in function of \u03c4. This plot allows to\nidentify different types of noise.\nTo accomplish this, we recorded data for 3 hfrom the\nthree IMUs of the Emlid Reach modules as well as from\nthe RealSense IMU. Next, we compute the Allan deviation\nfor both the gyroscope and accelerometer using the Allan\nVariance ROS Toolbox (Buchanan (2021)) and extract the\nfour key parameters from the resulting plots. Table 7 shows\nthe results. These parameters are required to calibrate camera\nand IMU jointly using Kalibr.\n3.5.2 Camera-IMU Calibration: Once we obtain the\naforementioned parameters from the Allan Variance method\nwe can jointly calibrate the stereo camera and IMUs.\nFor this step we use Kalibr (Rehder et al. (2016)). After\ncollecting enough visual-inertial measurements to perform\nthe calibration, we run Kalibr on this data and supplythe IMU intrinsic parameters as input. As a result of the\ncalibration, we obtained the camera intrinsic parameters and\nthe extrinsic transformations between the stereo camera and\nthe IMUs.\n3.5.3 Odometry Calibration: The Ackermann model\nparameters were manually determined for wheel diameter\nand wheelbase ( L), and semi-automatically for steering\nangles ( \u03b4). The procedure to obtain the steering angles\nconsists of first determining the diameter of the ground-truth\ncurves corresponding to the robot\u2019s maximum steering\nangles (hard right and hard left) and then using a physics-\nbased simulation of the robot, we perform a parameter\noptimization by iteratively adjusting the angle inputs to\nminimize the cost function defined as the difference between\nthe simulated and ground-truth trajectories. This is depicted\nin Figure 5.\n\u03b4\n\u03b4L\nFigure 5. Diagram of the trajectory of the calibration routine\nand parameters estimated by the odometry calibration.\n3.5.4 Extrinsic Calibration of Remaining Sensors:\nExtrinsic measurements for the remaining sensors were\ntaken manually. Figure 6 shows a 3D representation of\nthe different sensors frames of reference. The robot\u2019s\nbase_link was set at the mid-point of the rear wheel axes,\nand the sensor_box_link was set at the contact point\nfor the sensor suite box and the chassis, at the mid point of\nthe rear bottom-most point of the box.\n4 Data Collection\nThis section provides an overview of the dataset collected\nby the robot in the field, along with calibration data for\nthe sensors. We analyze the different recorded sequences,\npresenting the ground-truth, statistics and environment\nconditions.\nWe recorded six separate sequences of the robot traversing\na soybean plantation. The first three sequences (sequences\n#1 to #3) took place on the December 22nd, 2023, in a\nfield referred to as Field#1. The remaining three sequences\n(sequences #4 to #6) were conducted on the December 26th,\nPrepared using sagej.cls\n\n--- Page 7 ---\nThe Rosario Dataset v2 7\nr e a c h _ g p s 3\nr e a c h _ i m u 2\nr e a c h _ i m u 1\nr e a l s e n s e _ i m u\nr e a l s e n s e _ i n f r a 1 _ o p t i c a l _ f r a m e\nr e a l s e n s e _ i n f r a 2 _ o p t i c a l _ f r a m e\nr e a l s e n s e _ c o l o r _ o p t i c a l _ f r a m e\nr e a c h _ i m u 3r e a c h _ g p s 2r e a c h _ g p s 1\nb a s e _ l i n k\n s e n s o r _ b o x _ l i n k\nFigure 6. Transformations for the robot are shown overlaid onto a 3D representation of the robot, with two zoomed-in plots for the\nsensor box suite to better showcase its sensor and link frames. The RealSense optical frame names are stacked from left to right.\n2023, in a different field, approximately 830 m away from\nthe first, which we will call Field#2. Both fields belong to\nthe Faculty of Agronomy (National University of Rosario),\nlocated in Zavalla, Santa Fe, Argentina. On the first day\u2019s\nfield the rows of soybean plants were sowed with a spacing\nof0.52 m , whereas in the second day\u2019s field the rows were\nspaced by 0.42 m , both with none or minimum tilling. The\nplants were manually measured and had an average width\nof0.30 m and an average height of 0.40 m on both dates.\nThe plants were in a vegetative growth stage of V6 to V8\nin the first day, and V3 or V4 in the second day, where a\nvegetative stage of V imeans that the plant has itrifoliate\nleaves completely expanded. The stage of V8 precedes the\nplant flowering stage. Figure 7 shows close-up images of the\nsoybean plants and the disposition of the crop rows.\nFigure 8 shows the trajectories overlaid on a map, while\nTable 3 summarizes the key aspects of the six sequences.\nWe recorded a total of 2.23 h of trajectory, with a total\nlength of over 7.33 km , more than four times the duration of\nthe previous Rosario Dataset, and covering more than three\ntimes the distance.\nAt the beginning of sequences #1 and #4 (the first\nsequence for each distinct field) we performed a calibration\nroutine. This involved maneuvering the robot forwards and\nbackwards three times in a straight line and then making\ncircular patterns (first turning fully clockwise and then\nturning fully counter-clockwise) while capturing images of\ntwo Kalibr AprilTag grids. Such initial calibration pattern\nguarantees to have data across all axes in the 2D plane\nallowing, for instance, wheel odometry and magnetometer\nsensor calibration and SLAM system initialization. The\ndataset includes metadata that marks the time when the\ncalibration ends, allowing users to ignore this portion if they\nare not interested in the calibration data.\nThe recorded sequences were planned to contain\noverlapping trajectories, both in a given sequence and in\nbetween different sequences, to allow for testing of loop\ndetection and loop closure systems in these environments.\n(a)First Day (2023-12-22)\n (b)Second Day (2023-12-26)\nFigure 7. Sample images of the soybean crops from the two\ndays where the recordings took place. A zoom-in shows the\nplants in more detail, as well as some of the weeds present in\nthe field. The images were taken with a handheld camera.\nWe also made sure to record on sunny days, which provides\ngood natural illumination, an important factor for key-\npoint detection and a good system localization. Other\ncharacteristics of the sequences are that they always move\nforwards, except when we perform the calibration routines,\nas a typical agriculture machine would do, and that the terrain\nis rough, which presents some naturally occurring shakiness\nin the camera feed and the sensor readings. Figure 9 shows\nsample images from the stereo IR camera and the color\ncamera.\nThe robot\u2019s movement was manually controlled by an\noperator with a radio-frequency controller as described in\nSection 3.\nPrepared using sagej.cls\n\n--- Page 8 ---\n8 Journal Title XX(X)\nTrajectory #1\nTrajectory #2\nTrajectory #3\nTrajectory #4\nTrajectory #5\nTrajectory #6\nFigure 8. The six trajectories of the GNSS data from the dataset plotted onto a satellite map with zoomed out boxes singling out\neach trajectory individually.\nSequence Sequence ID Duration ( s) Distance ( m) Calibration Start ( s) Calibration End ( s)\n#1 2023-12-22-13-14-16 940 777 0 218\n#2 2023-12-22-14-29-43 1011 904 - -\n#3 2023-12-22-16-31-08 943 950 - -\n#4 2023-12-26-13-39-43 2506 2254 0 190\n#5 2023-12-26-15-10-15 796 703 - -\n#6 2023-12-26-15-48-38 1862 1744 - -\nTable 3. Characteristics of the six recorded sequences of the dataset.\n4.1 Ground-Truth\nIn addition to the GNSS-RTK positional solution, we\ncompute GNSS-PPK (Post-Processed Kinematic) solutions\nfor each Emlid Reach GNSS module to achieve higherprecision. All raw GNSS data was saved for both the\nbase station and the modules on board the rover. RINEX\nfiles stored by the Emlid Reach GNSS modules where\nPrepared using sagej.cls\n\n--- Page 9 ---\nThe Rosario Dataset v2 9\nFigure 9. Sample images from the RealSense D435i color camera and stereo infrared camera (left camera only), one pair for each\ntrajectory.\npost-process with the RTKLIB||, a open-source library for\nprocessing GNSS data that provides tools for computing\nGNSS-PPK solutions. We performed this procedure for each\nEmlid Reach device onboard the robot using all available\ncorrections from the base station module.\nThe 6-DoF inertial information provided by the Intel\nRealsense D435i IMU was later fused with the GNSS+PPK\nlocalization solutions using the MINS Lee et al. (2023)\nmultisensor fusion framework. Using the two GNSS-PPK\nsolutions from the Emlid Reach M2 modules, MINS is\nable to resolve a globally referenced orientation in a East-\nNorth-Up (ENU) global frame of the Intel Realsense D435i\nIMU coordinate frame system for every inertial reading.\nMeasurements from the Emlid Reach M1 module were\ndiscarded from the computation due to its difficulty in\nacquiring a fix and its inherent limitations as a single-band\nreceiver. During real-time operation, the M1 experienced\npoor SNR (signal-to-noise ratio) and struggled to maintain\na fix for a few minutes, resulting in measurements that were\nunreliable for fusion with the data from the other sensors.\nGround-truth localization information is finally generated\napplying the extrinsic transformation between the IMU\nframe and the robot body frame (base link frame).\nThe estimated standard deviation of the PPK solutions,\nas reported in the RTKLIB output during post-processing,\nis approximately 5 mm in the horizontal plane and 1 cm\nin altitude. These values are derived from the covariance\nreported by the internal EKF of RTKLIB and provide a\nreasonable indication of internal confidence in the solution.\nFurthermore, both PPK solutions used as input to MINS\nmaintained a Fix status 100 % of the time, ensuringmaximum confidence in their positional accuracy. When\ncomparing the MINS output with one of the original\nPPK trajectories, the mean positional difference varies\nacross sequences, typically ranging between 4 cm and8 cm .\nNotably, MINS provides a reliable estimate of orientation,\nwhich is not available from PPK alone. We are releasing\nboth the original PPK solutions and the MINS-derived\ntrajectories as part of the dataset, this ensures that users\ncan choose the most suitable reference for their needs and\nthe possibility of performing their own sensor fusion and\norientation estimation if desired.\n5 Experimental Evaluation\nTo validate the proposed dataset, we evaluate and\nanalyze state-of-the-art multi-modal SLAM systems. In\nparticular, we assess the stereo-inertial configuration of\nORB-SLAM3 (Campos et al. (2021)), the GNSS-stereo-\ninertial configuration of ORB-SLAM3+GNSS (Cremona\net al. (2023)) and the stereo-inertial implementation of\nOpenVINS (Geneva et al. (2020)). For ORB-SLAM3 we\nset the same parameters as in Cremona et al. (2022), as\nthey also evaluate the method on the previously released\nagricultural dataset \u201cThe Rosario Dataset\u201d. Also for ORB-\nSLAM3+GNSS we use the default parameters provided\nin the implementation released by Cremona et al. (2023),\nwhich was also tested on the former dataset. In the case of\nOpenVINS we set similar parameters to the other methods:\n1200 features extracted per image, with a state vector\nof 50 points and KLT tracking (as recommended by the\n\u2225https://github.com/rtklibexplorer/RTKLIB.git\nPrepared using sagej.cls\n\n--- Page 10 ---\n10 Journal Title XX(X)\nauthors). We use the RealSense D435i IR stereo camera and\nits IMU, and a single dual-band GNSS (an Emlid Reach\nM2) to run the systems. The results show that, initially,\nall systems estimate the robot trajectory properly, but as\nthe robot navigates for a while, pose estimation drifts or\neven loses localization. Although each system has a loop\ndetection module, we turn off the module to avoid false\npositive loops resulting from the perceptual aliasing nature\nof the environment, as reviewed in Soncini et al. (2024). To\njustify this decision we show an example trajectory obtained\nfrom running ORB-SLAM3 with loop closure enabled in\nFigure 10. In this trajectory the system closed 7 loops, with\nnegligible translation errors and maximum rotation error\nof170\u25e6(when comparing to ground-truth pose differences\nbetween the selected keyframes), irreversibly corrupting the\nestimated trajectory.\n0\n25\n50\n75\n100\n125\n150\nx (m )\n0\n10\n20y (m )\nGround Truth\nORB_SLAM3\nFigure 10. Trajectory obtained by running ORB-SLAM3 on\nsequence #1 with loop closure active. The system incorrectly\ncloses 7 loops and the estimated trajectory gets irreversibly\ncorrupted from them.\nFigure 11 shows the trajectories estimated by each\nsystem as the best alignment possible with the ground-\ntruth trajectory via Umeyama alignment Umeyama (1991).\nOpenVINS did not run on sequence #2 as it failed to initialize\nthe IMU for tracking. We omit the first few seconds for\nORB-SLAM3 in sequences #2, #3, #5 and #6, as it takes\nsome time for it to initialize correctly. In particular, sequence\n#2 takes the longest, at around 230 s to converge. ORB-\nSLAM3 also loses track in sequence #6 with about 300 s left\nto finish. It can be noted in the trajectories how the different\nmethods drift over time, with the one showing some of the\nbest trajectories being ORB-SLAM3+GNSS, as expected\ngiven it\u2019s the only one that fuses information from a GNSS\nsensor. The figures also show the start and end poses of\nthe robot for each trajectory as a similarly colored circle\nand cross respectively, where some zoomed-in clippings for\ntrajectories #1 and #4 are shown, as the path for the robot\nwas planned in such a way that the robot started and ended\nin the exact same place by adding stakes with red markers\non the ground where the four wheels should end up. We can\nsee by the results that in these two trajectories none of the\nmethods were able to perfectly start and end at the same time\ndue to the accumulated drift. It\u2019s our expectation that these\nissues could be mitigated by incorporating a loop closuresystem that works in agricultural fields, which is one of the\nmotivations for releasing this dataset.\nWe evaluate the accuracy of the estimated trajectories\nof each method by calculating two standard metrics Sturm\net al. (2012): the absolute pose error (APE) (also called\nabsolute trajectory error or ATE) which compares the\nabsolute distances between the estimated and ground-\ntruth trajectories, and the relative pose error (RPE) which\nmeasures the local accuracy of the trajectory over a fixed\ninterval (which in our case was set to 1 m displacement).\nSince our sensors are time-synchronized it\u2019s easy to pair\nthe poses of the estimated trajectories with our ground-\ntruth, we still need to align both trajectories to a common\nframe of reference, which is done with Umeyama alignment\nUmeyama (1991).\nTable 4 shows the APE results, where it can be seen that\nmethods significantly drift on all sequences of the dataset,\nwith particular issues when comparing the orientations of the\ntrajectories. A small amount of drift is expected for ORB-\nSLAM3 and OpenVINS given that these methods don\u2019t make\nuse of any global information (e.g. GNSS, compass, or loop-\nclosure systems), so as to make a fairer comparison we\nshowcase in Table 5 the RPE metrics calculated over the\nSLAM trajectories and ground-truth, which only makes local\ncomparisons over the estimated motion of the vehicle. These\nresults shows that agricultural environments, despite its\nimportance, are still challenging for state-of-the-art SLAM\nsystems.\n6 Summary and Future Work\nWe presented a calibrated, synchronized and tested dataset\naimed at evaluating and progressing the scope of autonomy\nin agricultural environments. It is our belief that this\ndataset is needed due to the scarcity of agricultural data\nfor autonomous operation on crop fields, and will help\nadvance many areas of robotics, control, and computer\nvision. This is no empty claim, as we provide experimental\nevaluations showing that current localization and mapping\nmethods are lackluster in these environments and need\nfurther improvement. Our results show that most of the issues\npointed out in Cremona et al. (2022) are still present in\nthe results produced by state-of-the-art SLAM systems and\nthus strengthens the importance of releasing the dataset and\nfurthering the research on the subject matter.\nIn the future we plan to keep working on enhancing the\ninformation available on the dataset by curating pose and\norientation ground-truths, enhancing the calibration results,\nannotating the visual cues and evaluating existing methods\nPrepared using sagej.cls\n\n--- Page 11 ---\nThe Rosario Dataset v2 11\n0\n50\n100\n150\n200\nx (m)\n0\n20y (m)\n-20 0 20 40 60 80 100 120 140\nx (m)010\n-10y (m)\n-175 -150 -125 -100 -75 -50 -25 0\nx (m)02040\n-20y (m)\n-150 -125 -100 -75 -50 -25 0\nx (m)-10010y (m)\n-150 -125 -100 -75 -50 -25 0\nx (m)020\n-204060y (m)0 50 100 150 200\nx  (m)-505y  (m)\nGround T ruth\nORB-SLAM3+GNSS\nORB-SLAM3\nOpenVINSTrajectory #1\nTrajectory #2\nTrajectory #3\nTrajectory #4\nTrajectory #5\nTrajectory #6\nFigure 11. Trajectories obtained by running diverse SLAM systems on the six sequences of the dataset. Some results are shown\npartially or not shown at all for lack of good pose tracking by such methods. Zoomed in portions of trajectories #1 and #4 are shown\naround the initial and final positions of each method as the robot was purposely made to start and finish on the same spot.\nPrepared using sagej.cls\n\n--- Page 12 ---\n12 Journal Title XX(X)\nMethodSequence\n#1 #2 #3 #4 #5 #6\nORB-SLAM35.17 m (2.44) 4.27 m (1.88)* 7.06 m (3.49)* 7.91 m (3.72) 1 m(0.54)* 1.83 m (0.88)*\n25.05\u25e6(0.94) 73.71\u25e6(0.36)* 2.68\u25e6(0.91)* 26.72\u25e6(1.94) 11\u25e6(0.26)* 1.64\u25e6(0.6)*\nORB-SLAM3+GNSS4.31 m (1.97) 1.83 m (0.90) 7.16 m (1.22) 7.12 m (3.94) 1.87 m (0.7) 1.86 m (0.99)\n40.67\u25e6(1.19) 29.89\u25e6(0.55) 34.96\u25e6(0.19) 49.77\u25e6(1.62) 9.01\u25e6(0.96) 17.63\u25e6(0.73)\nOpenVINS2.30 m (1.73) - 4.47 m (3.04) 3.48 m (2.53) 2.37 m (1.7) 4.37 m (3.11)\n4.91\u25e6(0.83) - 18.08\u25e6(0.35) 9.5\u25e6(0.66) 12.43\u25e6(0.31) 8.8\u25e6(0.34)\nTable 4. Mean and standard deviation (in parentheses) of the absolute pose error (APE) of the position and orientation of the\ntrajectories given by each method, when aligned to the ground-truth trajectory with Umeyama\u2019s method. Values preceded by an\nasterisk (*) were the result of truncated trajectories as described in the text.\nMethodSequence\n#1 #2 #3 #4 #5 #6\nORB-SLAM30.04 m (0.03) 0.09 m (0.04)* 0.06 m (0.05)* 0.04 m (0.03) 0.04 m (0.02)* 0.04 m (0.03)*\n0.20\u25e6(1.19) 0.16\u25e6(0.16)* 0.14\u25e6(0.12)* 0.17\u25e6(1.32) 0.12\u25e6(0.12)* 0.15\u25e6(0.15)*\nORB-SLAM3+GNSS0.04 m (0.03) 0.08 m (0.05) 0.04 m (0.06) 0.03 m (0.02) 0.04 m (0.04) 0.04 m (0.04)\n0.19\u25e6(1.45) 0.16\u25e6(1.2) 0.16\u25e6(1.03) 0.18\u25e6(1.14) 0.18\u25e6(2.24) 0.16\u25e6(0.15)\nOpenVINS0.04 m (0.03) - 0.07 m (0.08) 0.05 m (0.04) 0.05 m (0.03) 0.05 m (0.04)\n0.16\u25e6(0.15) - 0.16\u25e6(0.11) 0.14\u25e6(0.12) 0.13\u25e6(0.13) 0.15\u25e6(0.11)\nTable 5. Mean and standard deviation (in parentheses) of the relative pose error (RPE) of the position and orientation of pairs of\nposes at a distance of 1 mfor the trajectories given by each method. Values preceded by an asterisk (*) were the result of truncated\ntrajectories as described in the text.\nto further the work of autonomy on agricultural settings.\nWe also have plans to continue recording data on the field\nin coming years, making an attempt to capture different\nconditions of lighting, weather and crop growth stages,\nincorporating new sensor configurations, such as lidars and\nradars, and enhancing the measurement tools, such as more\nprecise imu and gnss modules.\nAcknowledgements\nWe specially thank Engr. N \u00b4estor Di Leo from the Land Management\nChair of the Faculty of Agricultural Sciences of the National\nUniversity of Rosario for giving us access to the agricultural field.\nFunding\nThis work was partially supported by Consejo Nacional de Inves-\ntigaciones Cient \u00b4\u0131ficas y T \u00b4ecnicas (Argentina) under grants PIBAA\nNo.0042, AGENCIA I+D+i (PICT 2021-570), and by Universidad\nNacional de Rosario (PCCT-UNR 80020220600072UR).\nReferences\nAit I, Kofman E and Pire T (2023) A Travelling Salesman Problem\nApproach to Efficiently Navigate Crop Row Fields with a Car-\nLike Robot. (IEEE) Latin America Transactions 21(5).\nBai Y , Zhang B, Xu N, Zhou J, Shi J and Diao Z (2023)\nVision-based navigation and guidance for agricultural\nautonomous vehicles and robots: A review. Computers\nand Electronics in Agriculture 205: 107584. DOI:\nhttps://doi.org/10.1016/j.compag.2022.107584. URLhttps://www.sciencedirect.com/science/\narticle/pii/S0168169922008924 .\nBender A, Whelan B and Sukkarieh S (2020) A high-\nresolution, multimodal data set for agricultural robotics:\nA Ladybird\u2019s-eye view of Brassica. Journal of Field\nRobotics 37(1): 73\u201396. DOI:https://doi.org/10.1002/rob.\n21877. URL https://onlinelibrary.wiley.com/\ndoi/abs/10.1002/rob.21877 .\nBuchanan R (2021) Allan variance ros. URL https://github.\ncom/ori-drs/allan_variance_ros .\nCampos C, Elvira R, Rodr \u00b4\u0131guez JJG, M Montiel JM and D Tard \u00b4os\nJ (2021) ORB-SLAM3: An Accurate Open-Source Library\nfor Visual, Visual\u2013Inertial, and Multimap SLAM. IEEE\nTrans. Robotics 37(6): 1874\u20131890. DOI:10.1109/TRO.2021.\n3075644.\nCa\u02dcnadas-Ar \u00b4anega F, Blanco-Claraco JL, Moreno JC and Rodriguez-\nDiaz F (2024) Multimodal Mobile Robotic Dataset for a\nTypical Mediterranean Greenhouse: The GREENBOT Dataset.\nSensors 24(6). DOI:10.3390/s24061874. URL https://\nwww.mdpi.com/1424-8220/24/6/1874 .\nChebrolu N, Lottes P, Schaefer A, Winterhalter W, Burgard\nW and Stachniss C (2017) Agricultural robot dataset for\nplant classification, localization and mapping on sugar beet\nfields. Intl. J. of Robotics Research 36(10): 1045\u20131052.\nDOI:10.1177/. URL https://doi.org/10.1177/\n0278364917720510 .\nCox J, Tsagkopoulos N, Rozsyp \u00b4alek Z, Krajn \u00b4\u0131k T, Sklar\nE and Hanheide M (2023) Visual teach and gener-\nalise (VTAG)\u2014Exploiting perceptual aliasing for scalable\nPrepared using sagej.cls\n\n--- Page 13 ---\nThe Rosario Dataset v2 13\nautonomous robotic navigation in horticultural environ-\nments. Computers and Electronics in Agriculture 212:\n108054. DOI:https://doi.org/10.1016/j.compag.2023.108054.\nURL https://www.sciencedirect.com/science/\narticle/pii/S0168169923004428 .\nCremona J, Civera J, Kofman E and Pire T (2023) GNSS-\nstereo-inertial SLAM for arable farming. Journal of\nField Robotics n/a(n/a). DOI:https://doi.org/10.1002/rob.\n22232. URL https://onlinelibrary.wiley.com/\ndoi/abs/10.1002/rob.22232 .\nCremona J, Comelli R and Pire T (2022) Experimental evaluation of\nVisual-Inertial Odometry systems for arable farming. Journal\nof Field Robotics 39(7): 1123\u20131137. DOI:10.1002/rob.\n22099. URL https://onlinelibrary.wiley.com/\ndoi/abs/10.1002/rob.22099 .\nCrocetti F, Bellocchio E, Dionigi A, Felicioni S, Costante\nG, Fravolini ML and Valigi P (2023) ARD-VO: Agri-\ncultural robot data set of vineyards and olive groves.\nJournal of Field Robotics DOI:https://doi.org/10.1002/rob.\n22179. URL https://onlinelibrary.wiley.com/\ndoi/abs/10.1002/rob.22179 .\nCuaran J, Velasquez AEB, Gasparino MV , Uppalapati NK,\nSivakumar AN, Wasserman J, Huzaifa M, Adve S and\nChowdhary G (2024) Under-canopy dataset for advancing\nsimultaneous localization and mapping in agricultural robotics.\nThe International Journal of Robotics Research 43(6): 739\u2013\n749. DOI:10.1177/02783649231215372. URL https://\ndoi.org/10.1177/02783649231215372 .\nDi Domenico F (2024) Estimaci \u00b4on de Orientaci \u00b4on Absoluta de un\nRobot Agr \u00b4\u0131cola Utilizando un Sensor MARG . Master\u2019s Thesis,\nFCEIA, Universidad Nacional de Rosario.\nDi Domenico F and Pire T (2023) Estimaci \u00b4on de orientaci \u00b4on de un\nrobot agr \u00b4\u0131cola utilizando el filtro de madgwick. In: Workshop\non Information Processing and Control (RPIC) . pp. 218\u2013223.\nFukase E and Martin W (2020) Economic growth, convergence,\nand world food demand and supply. World Development 132:\n104954. DOI:https://doi.org/10.1016/j.worlddev.2020.104954.\nURL https://www.sciencedirect.com/science/\narticle/pii/S0305750X20300802 .\nGeneva P, Eckenhoff K, Lee W, Yang Y and Huang G\n(2020) OpenVINS: A Research Platform for Visual-Inertial\nEstimation. In: IEEE Intl. Conf. on Robotics and Automation\n(ICRA) . pp. 4666\u20134672. DOI:10.1109/ICRA40945.2020.\n9196524.\nG\u00a8uldenring R, van Evert FK and Nalpantidis L (2023)\nRumexWeeds: A grassland dataset for agricultural robotics.\nJournal of Field Robotics 40(6): 1639\u20131656. DOI:https://doi.\norg/10.1002/rob.22196. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.22196 .\nIEEE-Standard (1998) IEEE Standard Specification Format Guide\nand Test Procedure for Single-Axis Interferometric Fiber Optic\nGyros. IEEE Std 952-1997 : 1\u201384DOI:10.1109/IEEESTD.\n1998.86153.\nKragh MF, Christiansen P, Laursen MS, Larsen M, Steen KA,\nGreen O, Karstoft H and J\u00f8rgensen RN (2017) FieldSAFE:\nDataset for Obstacle Detection in Agriculture. Sensors\n17(11). DOI:10.3390/s17112579. URL https://www.\nmdpi.com/1424-8220/17/11/2579 .\nLee W, Geneva P, Chen C and Huang G (2023) MINS: Efficient and\nRobust Multisensor-aided Inertial Navigation System. arXiv\npreprint arXiv:2309.15390 URL https://github.com/\nrpng/MINS .\nLiu Y , Fu Y , Qin M, Xu Y , Xu B, Chen F, Goossens B, Sun PZ, Yu\nH, Liu C, Chen L, Tao W and Zhao H (2024) BotanicGarden:\nA High-Quality Dataset for Robot Navigation in Unstructured\nNatural Environments. IEEE Robotics and Automation Letters\n9(3): 2798\u20132805. DOI:10.1109/LRA.2024.3359548.\nMueller A (2019) Modern Robotics: Mechanics, Planning, and\nControl. IEEE Control Systems Magazine 39(6): 100\u2013102.\nDOI:10.1109/MCS.2019.2937265.\nPire T, Mujica M, Civera J and Kofman E (2019) The Rosario\ndataset: Multisensor data for localization and mapping in\nagricultural environments. Intl. J. of Robotics Research 38(6):\n633\u2013641. DOI:10.1177/0278364919841437.\nPolvara R, Molina S, Hroob I, Papadimitriou A, Tsiolis\nK, Giakoumis D, Likothanassis S, Tzovaras D, Cielniak\nG and Hanheide M (2023) Bacchus Long-Term (BLT)\ndata set: Acquisition of the agricultural multimodal BLT\ndata set with automated robot deployment. Journal of\nField Robotics n/a(n/a). DOI:https://doi.org/10.1002/rob.\n22228. URL https://onlinelibrary.wiley.com/\ndoi/abs/10.1002/rob.22228 .\nRehder J, Nikolic J, Schneider T, Hinzmann T and Siegwart R\n(2016) Extending kalibr: Calibrating the extrinsics of multiple\nIMUs and of individual axes. In: 2016 IEEE International\nConference on Robotics and Automation (ICRA) . pp. 4304\u2013\n4311. DOI:10.1109/ICRA.2016.7487628.\nSa I, Popovi \u00b4c M, Khanna R, Chen Z, Lottes P, Liebisch F, Nieto\nJ, Stachniss C, Walter A and Siegwart R (2018) WeedMap:\nA Large-Scale Semantic Weed Mapping Framework Using\nAerial Multispectral Imaging and Deep Neural Network for\nPrecision Farming. Remote Sensing 10(9). URL https:\n//www.mdpi.com/2072-4292/10/9/1423 .\nSoncini N, Civera J and Pire T (2024) Addressing the challenges\nof loop detection in agricultural environments. Journal\nof Field Robotics n/a(n/a). DOI:https://doi.org/10.1002/rob.\nPrepared using sagej.cls\n\n--- Page 14 ---\n14 Journal Title XX(X)\n22414. URL https://onlinelibrary.wiley.com/\ndoi/abs/10.1002/rob.22414 .\nSturm J, Engelhard N, Endres F, Burgard W and Cremers D (2012)\nA benchmark for the evaluation of rgb-d slam systems. In: 2012\nIEEE/RSJ International Conference on Intelligent Robots and\nSystems . pp. 573\u2013580. DOI:10.1109/IROS.2012.6385773.\nTanco MM, Barnech GT, Andrade F, Baliosian J, LLofriu M,\nMartino JD and Tejera G (2024) MAgro dataset: A dataset\nfor simultaneous localization and mapping in agricultural\nenvironments. The International Journal of Robotics Research\n43(5): 591\u2013601. DOI:10.1177/02783649231210011. URL\nhttps://doi.org/10.1177/02783649231210011 .\nUmeyama S (1991) Least-squares estimation of transformation\nparameters between two point patterns. IEEE Transactions\non Pattern Analysis and Machine Intelligence 13(4): 376\u2013380.\nDOI:10.1109/34.88573.\nWoodman OJ (2007) An Introduction to Inertial Navigation. Tech-\nnical Report UCAM-CL-TR-696, University of Cambridge,\nComputer Laboratory.\nSupplemental material\n7 Appendix A: Calibration Parameters\nFor the RealSense D435i cameras we used a radial-tangential\ncalibration model that calibrates intrinsic parameters and distortion\ncoefficients for radial distortion and tangential distortion. The\nparameters are listed in Table 6 for all cameras.\nFor the RealSense D435i IMU and the Emlid Reach IMUs we\ncalibrated the Allan Variance, from which the values listed on\nTable 7 were obtained.\n8 Appendix B: Transformations\nTable 8 lists the rigid transformations between the different\ncoordinate frames involved in the system. Transformations are\ngiven by a combination of a translation vector (tx, ty, tz )and a\nrotation quaternion (qx, qy, qz, qw )(scalar-last).\nThe Git repository contains a convenient launch file that allows\nthetfmodule to automatically publish all these transformations\nfor use with ROS.\n9 Appendix C: Odometry\nWe get the raw data from the wheel motor\u2019s microcontroller\nthrough serial communication. These messages are recorded\nand made available in the dataset. The serial message has the\nform: ((< status > : [< motornr >, < rpm >, < pulses >\n, < duty >, < current > ]i:1...4, < angle >, < backward > )).Where status indicates whether the robot is in manual or automatic\nmode (all the sequences in this dataset are manual mode), motornr\nis an identifier of the wheel motor, rpm is the linear velocity of the\nwheel and pulses number of pulses detected by the sensors of the\nmotors since the system is started.\n10 Appendix D: Precision of PPS-Based\nTime Synchronization\n\u201cChrony\u201d**, the service we use to perform PPS-based\nsynchronization, provides precise offset measurements that allow\nus to evaluate synchronization performance. In this appendix, we\nanalyze the offset variations over each sequence to assess the\naccuracy and stability of time synchronization. The term offset\nrefers to the difference between the system current time and a\nreference time source measured by Chrony and considered to\ngradually converge the system clock with the reference. In this case,\nthe source is a GNSS receiver that, after convergence, achieves the\nprecision of an atomic clock from satellites.\nFigure 12 illustrates the distribution of offset values for each\nsequence. Samples with values exceeding 1.5times the interquartile\nrange (IQR) beyond the quartiles are shown separately, as is\ntypically done in a box plot. It should be mentioned that these\noutliers, marked as red crosses, are predominantly observed in the\nearlier intervals, suggesting a short period of instability (less than\n30 s) before the system reaches synchronization.\nDespite the presence of early outliers, the computed offsets for\neach sequence remain very small throughout the sequence, the\nmean being less than 0.32 milliseconds. This indicates that the\nsynchronization process is performing well, considering that the\nfastest sensor operates at 200Hz producing measurements every 5\nmilliseconds. Offset mean values shown confirm that the system\nclock remains closely aligned with the reference GNSS satellite\ntime effectively minimizes timing discrepancies, ensuring reliable\nand accurate timekeeping and stamping.\n11 Appendix E: File Structure\nWe release the rosbags for the 6 recorded sequences, as well as some\nadditional rosbags with postprocessed or independently recorded\ndata, that can be easily merged or played synchronously as the\ntimestamps line up.\n\u20222023_12_22_13_14_16.compressed.bag :\ncompressed rosbag for the first sequence of the first\nday (sequence #1),\n\u2217\u2217https://chrony-project.org/\nPrepared using sagej.cls\n\n--- Page 15 ---\nThe Rosario Dataset v2 15\nCameraIntrinsics Distortion\nfx fy cx cy k1 k2 p1 p2\ncolor 890.4202 895 .5269 633 .5761 375 .3947 0 .0511 \u22120.0881 \u22120.0011 \u22120.0004\ninfra1 645.4064 648 .5756 648 .7339 349 .0376 0 .0008 \u22120.0010 \u22120.0005 0 .0004\ninfra2 645.9158 649 .1492 647 .7810 348 .7846 0 .0005 \u22120.0005 \u22120.0005 0\nTable 6. RealSense D435i camera intrinsic parameters obtained with calibration, given with a precision of 8 decimal points.\nModuleAccelerometer Gyroscope\nNoise Density Random Walk Noise Density Random Walk\nRealSense D435i 0.00099438 0 .00004750 0 .00023249 0 .00000182\nEmlid Reach M1 0.00236037 0 .00016363 0 .00015210 0 .00001414\nEmlid Reach M2 (1) 0.00335015 0 .00011784 0 .00016841 0 .00002311\nEmlid Reach M2 (2) 0.00269792 0 .00026127 0 .00019469 0 .00001151\nTable 7. Noise values obtained from the calibration for the different IMU units, given with a precision of 8 decimal points.\nFrame ID Child Frame ID tx[m] ty[m] tz[m] qx qy qz qw\nbase link sensor boxlink 2.065 0 1 .105 0 0 0 1\nsensor boxlink reach 1 0.04 0 .4 0 .263 0 0 0 1\nsensor boxlink reach 2 0.04 \u22120.4 0 .263 0 0 0 1\nsensor boxlink reach 3 0.3825 0 0 .239 0 0 0 1\nsensor boxlink realsense bottom screw frame 0.312269 0 0 .051969 0 0 .1564345 0 0 .9876883\nrealsense bottom screw frame realsense infra1 optical frame \u22120.011 0 .018 0 .013 0.5 \u22120.5 0 .5 \u22120.5\nrealsense infra1 optical frame realsense infra2 optical frame 0.050240 0 .000032 0 .000181 0.0000482 \u22120.0006391 \u22120.0000144 0 .99999999\nrealsense infra2 optical frame realsense color optical frame \u22120.06363861 \u22120.00066263 \u22120.00010325 \u22120.00170204 \u22120.00387651 0 .00520012 0 .99997752\nrealsense infra1 optical frame realsense imu optical frame 0.004398 \u22120.019334 \u22120.027490 \u22120.002505 0 .000293 \u22120.001948 0 .999994\nrealsense imu optical frame reach 1imu \u22120.03328264 \u22120.09014729 \u22120.12951963 \u22120.41522836 0 .42277546 0 .57759602 0 .56145272\nrealsense imu optical frame reach 2imu 0.05413669 \u22120.05020996 \u22120.1210384 \u22120.4082114 \u22120.5625849 0 .43126286 0 .57521651\nrealsense imu optical frame reach 3imu \u22120.02761944 \u22120.04919555 \u22120.12547842 \u22120.10502215 \u22120.1182785 0 .6993272 \u22120.69708107\nTable 8. Rigid transformation between coordinate frames.\nFigure 12. Distribution of offset values for each sequence in the PPS-based synchronization process. Outliers are marked as red\ncrosses.\n\u20222023_12_22_14_29_43.compressed.bag :\ncompressed rosbag for the second sequence of the\nfirst day (sequence #2),\u20222023_12_22_16_31_08.compressed.bag :\ncompressed rosbag for the third sequence of the first\nday (sequence #3),\nPrepared using sagej.cls\n\n--- Page 16 ---\n16 Journal Title XX(X)\n\u20222023_12_26_13_39_43.compressed.bag :\ncompressed rosbag for the first sequence of the second day\n(sequence #4),\n\u20222023_12_26_15_10_15.compressed.bag :\ncompressed rosbag for the second sequence of the\nsecond day (sequence #5),\n\u20222023_12_26_15_48_38.compressed.bag :\ncompressed rosbag for the third sequence of the second day\n(sequence #6),\n\u2022*_conventional_gnss.bag : rosbags with conven-\ntional GNSS data (no post-processing) for the corresponding\nsequence,\n\u2022*_ppk_gnss.bag : rosbags with post-processed (PPK)\nGNSS data for the corresponding sequence,\n\u2022*_pgt.bag : rosbags with post-processed ground-truth\ninformation as described in Section 4 for the corresponding\nsequence .\nThese rosbags contain collision-free names for the different\ntopics, making running or merging multiple rosbags hassle free.\nScripts are provided in the linked GitHub repository that\nallow extracting any and all rosbags into a common file\ndirectory structure, of images and comma-separated-value files, and\nconverting between common file representations where possible.\nWe also provide the rosbags recording for the calibration of\nthe different sensors and the files resulting from running such\ncalibrations:\n\u2022reach_imus_bias.bag : rosbag with a 3 hrecording of\nthe data produced by the IMU units of the Reach sensors\nwhen stationary, useful for calculating the biases,\n\u2022reach_imu_biases.tar.gz : compressed files result-\ning from running the Allan Variance ROS Compatible Tool\u2020\u2020\non the previously mentioned rosbag,\n\u2022realsense_imus_bias.bag : rosbag with a 3 hrecord-\ning of the data produced by the IMU unit of the RealSense\nD435i sensor when stationary, useful for calculating its bias,\n\u2022reach_imu_biases.tar.gz : compressed files result-\ning from running the Allan Variance ROS Compatible Tool\non the previously mentioned rosbag,\n\u2022allan_variance_config.tar.gz : compressed files\nused for configuration of the Allan Variance ROS\nCompatible Tool,\n\u2022aprilgrid_calibration.bag : rosbag with a calibra-\ntion routine recorded with an aprilgrid board and making\nsure to excite all axes, useful for calibrating with Kalibr,\n\u2022kalibr_ir_color.tar.gz : compressed files resulting\nfrom running the Kalibr Visual-Inertial Calibration Tool-\nbox\u2021\u2021on the previously mentioned rosbag. This calibrationwas performed using the IR stereo left camera and the color\ncamera of the RealSense D435i only,\n\u2022kalibr_ir_stereo.tar.gz : compressed files result-\ning from running the Kalibr Visual-Inertial Calibration Tool-\nbox on the previously mentioned rosbag, using only the IR\nstereo left and right cameras of the RealSense D435i,\n\u2022kalibr_ir_stereo_imu.tar.gz : compressed files\nresulting from running the Kalibr Visual-Inertial Calibration\nToolbox on the previously mentioned rosbag. This cali-\nbration was performed using the IR stereo left and right\ncameras, along with the IMU of the RealSense D435i only,\n\u2022kalibr_ir_stereo_imu_imus.tar.gz :\ncompressed files resulting from running the Kalibr\nVisual-Inertial Calibration Toolbox on the previously\nmentioned rosbag. This calibration was performed using the\nIR stereo left and right cameras of the RealSense D435i, its\nIMU, as well as the IMUs of the Reach sensors,\n\u2022kalibr_config.tar.gz : compressed files used for\nconfiguration of the Kalibr Visual-Inertial Calibration\nToolbox,\n\u2022magnetic_compass.tar.gz : compressed files con-\ntaining information about the magnetic compass, including\nmagnetic declination (for different magnetic models) and\ncalibrations generated from the trajectory sequences.\n11.1 Rosbag Topics\nIn Table 9 we list the different topics contained in the rosbags, their\nmessage types and a short description.\n12 Appendix F: Supplementary Dataset\nAs a preliminary step in recording this dataset, we collected a large\namount of agricultural data in previous field trips. This data can be\nof great utility for evaluating SLAM algorithms, sensor fusion, path\nplanning, navigation and control. For this reason we have decided\nto release the data captured in these field trips. In this appendix,\nwe provide detailed information about this supplementary dataset,\nhighlighting the differences between it and the core data presented\nin this article.\n12.1 The Robot\nThe main difference with respect to The Rosario Dataset 2023 is\nthe sensors mounted onboard the robot. Figure 13 shows the weed\nremoving robot at the time of recording the supplementary dataset.\nAs can be seen, the sensor layout differs here, as there is no sensor\n\u2020\u2020https://github.com/ori-drs/allan_variance_ros\n\u2021\u2021https://github.com/ethz-asl/kalibr\nPrepared using sagej.cls\n\n--- Page 17 ---\nThe Rosario Dataset v2 17\nTopic Name Topic Type Topic Description\n/distance wheel odometry/Distances Raw data from the wheel motor\u2019s microcontroller\n/odom nav msgs/OdometryWheel odometry,\nlinear and angular velocity\n/reach */fix sensor msgs/NavSatFixPosition data from the given\nReal-Time Kinematic GNSS (RTK)\n/reach */vel geometry msgs/TwistStampedVelocity data provided by the\ngiven GNSS module\n/reach */imu sensor msgs/ImuInertial measurement unit data\nfrom the given GNSS module\n/reach */imu/mag sensor msgs/MagneticFieldMagnetometer data from the\ngiven GNSS module\n/reach */imu/driver t sensor msgs/TimeReferenceTimestamps generated by the internal hardware\ndriver corresponding to the IMU measurements\n/reach */imu/system t sensor msgs/TimeReferenceTimestamps registered by the Emlid Reach module\nsystem clock corresponding to the IMU measurements\n/reach */time reference sensor msgs/TimeReference Timestamps from NMEA sentence of GNSS solution\n/realsense/infra1/camera info sensor msgs/CameraInfoIR camera parameters published\nby the RealSense ROS1 driver\n/realsense/infra1/image rectraw sensor msgs/ImageIR camera image published by\nthe RealSense ROS1 driver\n/realsense/infra2/camera info sensor msgs/CameraInfoIR camera parameters published\nby the RealSense ROS1 driver\n/realsense/infra2/image rectraw sensor msgs/ImageIR camera image published by\nthe RealSense ROS1 driver\n/realsense/color/camera info sensor msgs/CameraInfoColor camera parameters published\nby the RealSense ROS1 driver\n/realsense/color/image raw sensor msgs/ImageColor camera image published by\nthe RealSense ROS1 driver\n/realsense/depth/camera info sensor msgs/CameraInfoDepth camera parameters published\nby the RealSense ROS1 driver\n/realsense/depth/image rectraw sensor msgs/ImageIR depth image published by\nthe RealSense ROS1 driver\n/realsense/accel/imu info realsense2 camera/IMUInfoAccelerometer parameters published\nby the RealSense ROS1 driver\n/realsense/gyro/imu info realsense2 camera/IMUInfoGyroscope parameters published\nby the RealSense ROS1 driver\n/realsense/imu sensor msgs/ImuInertial measurement unit data published\nby the RealSense ROS1 driver\n/tf tf2 msgs/TFMessage Dynamic transformations between coordinate frames\n/tfstatic tf2 msgs/TFMessage Static transformations between coordinate frames\n/reach */gps/fix sensor msgs/NavSatFix Single Point Positioning GNSS (SPP)\n/reach */gps/vel geometry msgs/TwistStamped Velocity data from SPP GNSS\n/reach */ppk/fix sensor msgs/NavSatFix Post-Processed Kinematic GNSS (PPK)\n/reach */ppk/vel geometry msgs/TwistStamped Velocity data from PPK GNSS\nTable 9. Recorded or post-processed topic names, their associated type, and a description of the data it contains. An asterisk\nimplies that there are multiple topics similarly named with the same type and description.\nbox. To capture the data, the sensors are connected to a NVIDIA\nJetson TX2. The rest of the robotic platform is the same as described\nin Section 3.\n12.2 Data Collection Systems Overview\nThe robot navigation and data recording stack was implemented on\nthe ROS framework. For each sensor there is a ROS node capable of\ntaking data from the sensor and converting it to a ROS message. If\navailable, we use the official implementation of the node provided\nby the sensor manufacturer. All the messages coming from thesensors are stored in a single rosbag for each trajectory performed\nexcept for the images. Due to the size of the disk space occupied\nby the images, we chose to use the SVO format provided by the\nZed camera and created by StereoLabs. This format compresses\nthe images captured by the camera allowing them to be stored with\nsignificantly less disk space. Tools that allow conversion from SVO\nto rosbag are available.\nPrepared using sagej.cls\n\n--- Page 18 ---\n18 Journal Title XX(X)\nName SensorResolution /\nRangeAcquisition Rate\nZed Camera Stereo Color Camera1280 \u00d7 720 and 672 x 376\n90\u25e6\u00d760\u25e6 15 Hz\nEmlid Reach M1SPP \u00b1 2.5 m 5 Hz\nRTK-GNSS \u00b1 0.04 m 5 Hz\n6-DoF IMU\u00b18g\n\u00b11000 deg /s200 Hz\nMagnetometer - 40 Hz\nE-Bike Wheel\nOdometerHall-Effect Odometry \u00b1 7.5\u25e610 Hz\nOMRON\nE6CP-A8-bit Absolute Encoder\u00b11.4\u25e6\n92\u25e6 10 Hz\nTable 10. List of sensors mounted on the platform at the time of recording the supplementary dataset.\nFigure 13. Weed removing robot at the time of recording the\nsupplementary dataset.\n12.3 Dataset\nThe tests were conducted on December 28th and 29th, 2021.\nHenceforth, we will refer to these working days as Day 1 and Day\n2, respectively. Seven recording sessions were conducted between\nthe two days, totalling seven data sequences. The sequences are\nnumbered in chronological order, with sequence 1 being the first\nsequence recorded on Day 1 and sequence 7 the last sequence\ncaptured on Day 2. For all trajectories, data from the stereo\ncameras, IMU, magnetometer, GNSS-RTK, velocity from GNSS,\nSPP from GNSS, and wheel odometry are captured and recorded.\nAdditionally, raw data provided by the GNSS receiver modules are\nstored. This data is processed offline using PPK to obtain a higher\nGNSS position accuracy than that achieved with RTK.\nOn Day 1 of the fieldwork, two distinct trajectories were\nperformed in the soybean field, resulting in Sequences 1 and 2.\nAt the start of Sequence 1, calibration routines were conducted,involving the robot rotating in circles in both directions, as\ndescribed in Section 4 for the 2023 dataset. On Day 2 of fieldwork,\nfive data sequences were collected (Sequences 3, 4, 5, 6, and 7).\nSimilar to Sequence 1, Sequences 3 and 4 begin with calibration\nroutines.\n12.4 Synchronization\nEach sensor measurement includes a timestamp. In this setup of the\nweed removing robot, the sensors were not hardware-synchronized.\nSLAM algorithms generally assume that input data is synchronized\non a single clock, so this lack of synchronization poses a challenge\nwhen using our data with SLAM algorithms.\nTo implement software-based synchronization, we identify\nmoments in the sequence where the robot remains stationary\nfor at least nseconds and then begins moving, detecting these\ntimestamps with each sensor involved. We then select one sensor\nas the reference and apply an offset to each of the other sensors\nto align them with the reference sensor. Although the absolute\ntiming might not be precise, our primary goal is to correct the\nrelative timing between sensors. The IMU is chosen as the reference\nsensor because its high message frequency provides greater timing\ngranularity.\n12.5 Ground-Truth\nHaving a high-precision ground-truth is essential for evaluating\nSLAM systems, as it allows for comparing SLAM estimates against\na reference trajectory. We planned to estimate PPK as outlined\nin Section 4.1. However, an issue with the base station in one\nrecording session on Day 1 caused it to shut down and stop\ntransmitting corrections.\nWith the aim of addressing this issue, we utilized a station from\nthe RAMSAC network for receiving the corrections. RAMSAC is\na network of GNSS reference stations across Argentina, managed\nby the Argentine National Geographic Institute (IGN), providing\na precise and uniform geodetic reference framework nationwide.\nPrepared using sagej.cls\n\n--- Page 19 ---\nThe Rosario Dataset v2 19\nThis network, with stations distributed across Argentina, supports\nreal-time monitoring and correction of GNSS data and is suitable\nfor both RTK and PPK applications. Each station in the network\nis equipped with high-precision GNSS receivers that record data\ncontinuously, ensuring a constant stream of information.\nThe nearest RAMSAC station to our fieldwork site is the UNRO\nstation, located at National University of Rosario, approximately\n25 kilometers from the soybean field. Although this distance is\nat the optimal range limit ( 20 km for single-frequency receivers,\naccording to RAMSAC website), the raw GNSS data from this\nstation was used only to calculate PPK for the few minutes of\nrecording when our local base station stopped transmitting. For all\nother data, PPK was processed using measurements from the local\nbase station whenever available.\nFor orientation ground-truth, we rely on the magnetometer and\nIMU measurements. Specifically, we perform a 2D magnetometer\ncalibration using data recorded during the calibration routines. We\nthen correct the magnetic measurements and estimate the global\norientation using a Madgwick filter Di Domenico and Pire (2023);\nDi Domenico (2024).\nPrepared using sagej.cls",
  "project_dir": "artifacts/projects/enhanced_cs.RO_2508.21635v1_The_Rosario_Dataset_v2_Multimodal_Dataset_for_Agr",
  "communication_dir": "artifacts/projects/enhanced_cs.RO_2508.21635v1_The_Rosario_Dataset_v2_Multimodal_Dataset_for_Agr/.agent_comm",
  "assigned_at": "2025-09-03T20:52:51.613057",
  "status": "assigned"
}